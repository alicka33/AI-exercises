{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 5\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski 1.[5 pkt]\n",
        "4. Jakość kodu 0.[5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) ->np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size:int, output_size:int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        pass\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        pass\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        pass\n",
        "\n",
        "    def loss_derivative(self, x:np.ndarray, y:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        pass\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss:Loss)->None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        pass\n",
        "\n",
        "    def fit(self,\n",
        "            x_train:np.ndarray,\n",
        "            y_train:np.ndarray,\n",
        "            epochs:int,\n",
        "            learning_rate:float,\n",
        "            verbose:int=0)->None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jonczyk/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 165\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# # Example usage:\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# # Define the layers\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# input_size = 2\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Load MNIST data\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m mnist \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_openml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist_784\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m X \u001b[38;5;241m=\u001b[39m mnist\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m    167\u001b[0m y \u001b[38;5;241m=\u001b[39m LabelBinarizer()\u001b[38;5;241m.\u001b[39mfit_transform(mnist\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1128\u001b[0m, in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# obtain the data\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m url \u001b[38;5;241m=\u001b[39m _DATA_FILE\u001b[38;5;241m.\u001b[39mformat(data_description[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 1128\u001b[0m bunch \u001b[38;5;241m=\u001b[39m \u001b[43m_download_data_to_bunch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mas_frame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_description\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5_checksum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_X_y:\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bunch\u001b[38;5;241m.\u001b[39mdata, bunch\u001b[38;5;241m.\u001b[39mtarget\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:677\u001b[0m, in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserError\n\u001b[1;32m    675\u001b[0m     no_retry_exception \u001b[38;5;241m=\u001b[39m ParserError\n\u001b[0;32m--> 677\u001b[0m X, y, frame, categories \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_with_clean_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_retry_exception\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_load_arff_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_names_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Bunch(\n\u001b[1;32m    695\u001b[0m     data\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    696\u001b[0m     target\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m     target_names\u001b[38;5;241m=\u001b[39mtarget_columns,\n\u001b[1;32m    701\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:67\u001b[0m, in \u001b[0;36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:542\u001b[0m, in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m arff_params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    533\u001b[0m     parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    534\u001b[0m     output_type\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     read_csv_kwargs\u001b[38;5;241m=\u001b[39mread_csv_kwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    540\u001b[0m )\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     X, y, frame, categories \u001b[38;5;241m=\u001b[39m \u001b[43m_open_url_and_load_gzip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marff_params\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:530\u001b[0m, in \u001b[0;36m_load_arff_response.<locals>._open_url_and_load_gzip_file\u001b[0;34m(url, data_home, n_retries, delay, arff_params)\u001b[0m\n\u001b[1;32m    528\u001b[0m gzip_file \u001b[38;5;241m=\u001b[39m _open_openml_url(url, data_home, n_retries\u001b[38;5;241m=\u001b[39mn_retries, delay\u001b[38;5;241m=\u001b[39mdelay)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m closing(gzip_file):\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_arff_from_gzip_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgzip_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marff_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_arff_parser.py:520\u001b[0m, in \u001b[0;36mload_arff_from_gzip_file\u001b[0;34m(gzip_file, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a compressed ARFF file using a given parser.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    `output_array_type == \"pandas\"`.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliac-arff\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_liac_arff_parser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgzip_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names_to_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_names_to_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_arff_parser(\n\u001b[1;32m    530\u001b[0m         gzip_file,\n\u001b[1;32m    531\u001b[0m         output_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         read_csv_kwargs,\n\u001b[1;32m    536\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/_arff_parser.py:197\u001b[0m, in \u001b[0;36m_liac_arff_parser\u001b[0;34m(gzip_file, output_arrays_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape)\u001b[0m\n\u001b[1;32m    195\u001b[0m columns_to_keep \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns_names \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns_to_select]\n\u001b[1;32m    196\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [first_df[columns_to_keep]]\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m _chunk_generator(arff_container[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunksize):\n\u001b[1;32m    198\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    199\u001b[0m         pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumns_names, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[columns_to_keep]\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# dfs[0] contains only one row, which may not have enough data to infer to\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# column's dtype. Here we use `dfs[1]` to configure the dtype in dfs[0]\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/__init__.py:729\u001b[0m, in \u001b[0;36m_chunk_generator\u001b[0;34m(gen, chunksize)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03mchunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/externals/_arff.py:462\u001b[0m, in \u001b[0;36mDenseGeneratorData.decode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_rows\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream, conversors):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m--> 462\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m values \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(values) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(conversors):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/externals/_arff.py:289\u001b[0m, in \u001b[0;36m_parse_values\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''(INTERNAL) Split a line into a list of values'''\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _RE_NONTRIVIAL_DATA\u001b[38;5;241m.\u001b[39msearch(s):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Fast path for trivial cases (unfortunately we have to handle missing\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# values because of the empty string case :(.)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m s\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# _RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m values, errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m_RE_DENSE_VALUES\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m s))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "class Layer(ABC):\n",
        "    def __init__(self):\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert 0 < learning_rate < 1, f\"Given learning_rate={learning_rate} is not in the range (0, 1)\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size, output_size)\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.input = x\n",
        "        return np.dot(x, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        weights_derivative = np.dot(self.input.T, output_error_derivative)\n",
        "        self.weights -= self.learning_rate * weights_derivative\n",
        "        self.bias -= self.learning_rate * np.sum(output_error_derivative, axis=0, keepdims=True)\n",
        "        return np.dot(output_error_derivative, self.weights.T)\n",
        "        \n",
        "    # class FullyConnected(Layer):\n",
        "    #     def __init__(self, input_size: int, output_size: int):\n",
        "    #         super().__init__()\n",
        "    #         self.input_size = input_size\n",
        "    #         self.output_size = output_size\n",
        "    #         self.weights = np.random.randn(input_size, output_size)\n",
        "    #         self.bias = np.zeros((1, output_size))\n",
        "    #         self.input = None\n",
        "\n",
        "    #     def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "    #         self.input = x\n",
        "    #         return np.dot(x, self.weights) + self.bias\n",
        "\n",
        "    #     def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "    #         weights_derivative = np.dot(self.input.T, output_error_derivative)\n",
        "    #         self.weights -= self.learning_rate * weights_derivative / len(self.input)\n",
        "    #         self.bias -= self.learning_rate * np.sum(output_error_derivative, axis=0, keepdims=True)\n",
        "    #         return np.dot(output_error_derivative, self.weights.T)\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.output = np.tanh(x)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error_derivative: np.ndarray) -> np.ndarray:\n",
        "        return output_error_derivative * (1 - np.square(self.output))\n",
        "\n",
        "class Loss(ABC):\n",
        "    @abstractmethod\n",
        "    def loss(self, predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss_derivative(self, predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "class MeanSquaredError(Loss):\n",
        "    def loss(self, predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "        return 0.5 * np.mean(np.square(predictions - targets))\n",
        "\n",
        "    def loss_derivative(self, predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "        return predictions - targets\n",
        "\n",
        "# ///////////////////////////////////////////////////////////////////////////////////////////////\n",
        "# def mean_squared_error_loss(predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "#     return 0.5 * np.mean(np.square(predictions - targets))\n",
        "\n",
        "# def mean_squared_error_derivative(predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
        "#     return predictions - targets\n",
        "\n",
        "# def loss(x: np.ndarray, y: np.ndarray, loss_function: callable) -> np.ndarray:\n",
        "#     \"\"\"Loss function for a particular x and y\"\"\"\n",
        "#     return loss_function(x, y)\n",
        "\n",
        "# def loss_derivative(x: np.ndarray, y: np.ndarray, loss_function_derivative: callable) -> np.ndarray:\n",
        "#     \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "#     return loss_function_derivative(x, y)\n",
        "\n",
        "# ///////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers: List[Layer], learning_rate: float):\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = None\n",
        "\n",
        "    def compile(self, loss: Loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def fit(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, verbose: int = 0):\n",
        "        for epoch in range(epochs):\n",
        "            predictions = self(x_train)\n",
        "            error = self.loss.loss(predictions, y_train)\n",
        "            error_derivative = self.loss.loss_derivative(predictions, y_train)\n",
        "\n",
        "            for layer in reversed(self.layers):\n",
        "                error_derivative = layer.backward(error_derivative)\n",
        "\n",
        "            if verbose and epoch % verbose == 0:\n",
        "                print(f\"Epoch: {epoch}, Error: {error}\")\n",
        "\n",
        "# # Example usage:\n",
        "# # Define the layers\n",
        "# input_size = 2\n",
        "# hidden_size = 4\n",
        "# output_size = 1\n",
        "\n",
        "# fully_connected_layer1 = FullyConnected(input_size, hidden_size)\n",
        "# tanh_layer = Tanh()\n",
        "# fully_connected_layer2 = FullyConnected(hidden_size, output_size)\n",
        "\n",
        "# #  mogę usunąć całą than layer jesli odrazu będę przemnażać przez funckję aktywacji !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# # Create the network\n",
        "# network = Network(layers=[fully_connected_layer1, tanh_layer, fully_connected_layer2], learning_rate=0.01)\n",
        "\n",
        "# # Compile the network with MeanSquaredError loss\n",
        "# network.compile(loss=MeanSquaredError())\n",
        "\n",
        "# # Define the training data\n",
        "# x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "# y_train = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# # Train the network\n",
        "# network.fit(x_train, y_train, epochs=1000, verbose=100)\n",
        "\n",
        "\n",
        "# Load MNIST data\n",
        "mnist = fetch_openml('mnist_784')\n",
        "X = mnist.data.astype('float32') / 255.0\n",
        "y = LabelBinarizer().fit_transform(mnist.target.astype('int'))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define your neural network layers\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "# Modify the layers as needed for your specific task\n",
        "layers = [\n",
        "    FullyConnected(input_size, 128),\n",
        "    Tanh(),\n",
        "    FullyConnected(128, output_size)\n",
        "]\n",
        "\n",
        "# Create the neural network\n",
        "learning_rate = 0.01\n",
        "network = Network(layers, learning_rate)\n",
        "\n",
        "# Compile the network with a loss function\n",
        "loss_function = MeanSquaredError()\n",
        "network.compile(loss_function)\n",
        "\n",
        "# Train the network\n",
        "epochs = 10\n",
        "verbose = 1\n",
        "network.fit(X_train, y_train, epochs, verbose)\n",
        "\n",
        "# Evaluate the network on the test set\n",
        "predictions = network(X_test)\n",
        "test_error = loss_function.loss(predictions, y_test)\n",
        "print(f\"Test Error: {test_error}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) ->np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size:int, output_size:int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size, output_size)\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "        # self.input = None\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        # self.input = x\n",
        "        return self.tanh.forward(np.dot(x, self.weights) + self.bias)  # nwm z tym , ale trzeba jeszcze przemnożyć przez ten tangens \n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        pass\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        pass\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        pass\n",
        "\n",
        "    def loss_derivative(self, x:np.ndarray, y:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        pass\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss:Loss)->None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def fit(self,\n",
        "            x_train:np.ndarray,\n",
        "            y_train:np.ndarray,\n",
        "            epochs:int,\n",
        "            learning_rate:float,\n",
        "            verbose:int=0)->None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            predictions = self(x_train)\n",
        "            erorr = self.loss.loss()\n",
        "            # error = self.loss.loss(predictions, y_train)\n",
        "            # error_derivative = self.loss.loss_derivative(predictions, y_train)\n",
        "\n",
        "            for layer in reversed(self.layers):\n",
        "                error_derivative = layer.backward(error_derivative)\n",
        "\n",
        "            if verbose and epoch % verbose == 0:\n",
        "                print(f\"Epoch: {epoch}, Error: {error}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
