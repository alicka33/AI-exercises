{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[6.3 2.5 4.9 1.5]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.  6.1 2.3]]\n"
          ]
        }
      ],
      "source": [
        "# print(iris.data)\n",
        "# print(iris.target)\n",
        "print(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples):\n",
        "\n",
        "    # num_samples = number of samples -> ilość wszystkich próbek \n",
        "    # class count -> ilość klass? ---- np klasa 0, 1, 2\n",
        "    # tyle razy będzie trzeba powtórzy pętle zliczając ile jest w tabeli wystąpień dla 0, potem dla 1, 2 --> nie wiem czy to tu czy krok wcześniej \n",
        "    # jak zliczymy ilość wystąpień to obliczamy prawdopodobieństwo na podstawie tego wyliczonego / num_samples (ilości wszystkich wierszy w grupie/ wielkości grupy)\n",
        "    # stosujemy wzór z logarytmem (pamiętaj o - bo nie uwzględniłąm go w klasie group)\n",
        "    # i dodajemy do sumy \n",
        "    probability = class_count/num_samples\n",
        "    return -probability * math.log2(probability) if probability > 0 else 0\n",
        "\n",
        "\n",
        "class Group:\n",
        "    def __init__(self, group_classes):\n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.group_classes.size\n",
        "\n",
        "    # liczy entropie grupy \n",
        "    def group_entropy(self):\n",
        "        entropy = 0\n",
        "        counter = Counter(self.group_classes)\n",
        "        for value, count in counter.items():\n",
        "            class_count = self.group_classes.count(count)\n",
        "            entropy += entropy_func(class_count, self.__len__()) # nie jestem pewna czy to len tak może być zapisane ale zobaczymy \n",
        "        return entropy\n",
        "        # policzyć ilośc wystąpienia danej klasy = class_count (np. klasy 0, albo 1 itd.) w group klasses, informacje o klasach dla tej grupy w groyp classes\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, split_feature, split_val, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.depth = depth\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val # może wartość wyliczona przez entropię?\n",
        "\n",
        "    def predict(self, data):\n",
        "        pass\n",
        "\n",
        "\n",
        "class DecisionTreeClassifier(object):\n",
        "    def __init__(self, max_depth):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a, group_b):\n",
        "        pass\n",
        "\n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        # ta funkcja musi się wykonywać dla kolejych kolumn - atrybutów, w jednej z funckji poniżej \n",
        "        parent_entropy = parent_group.group_entropy()\n",
        "        child_a_entropy = child_group_a.group_entropy()\n",
        "        child_b_entropy =  child_group_b.group_entropy()\n",
        "        child_a_parameter = child_group_a.__len__() / parent_group.__len__()\n",
        "        child_b_parameter = child_group_b.__len__() / parent_group.__len__()\n",
        "        # return ent(p) - (ilość wiersszy ch_a/ilość wierszy parent * ent(child_1) + (to samo dla b) )\n",
        "        return parent_entropy - child_a_parameter * child_a_entropy - child_b_parameter * child_b_entropy\n",
        "        \n",
        "\n",
        "    def get_best_feature_split(self, feature_values, classes):\n",
        "        # czy feature values to poprostu numer kolumny???\n",
        "\n",
        "        # tu może tworzyć obiekty klasy gruop \n",
        "        pass\n",
        "\n",
        "    def get_best_split(self, data, classes):\n",
        "        pass\n",
        "\n",
        "    def build_tree(self, data, classes, depth=0):\n",
        "        # jak tutaj zmieniać dane w date i klases orzy rekurencji ?\n",
        "        pass\n",
        "\n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)\n",
        "    def print_tree(self):\n",
        "        def print_node(node: Node, indent=\"\"):\n",
        "            if node.child_node_a is None and node.child_node_b is None:\n",
        "                print(indent + f\"Answer is {node.val}\")\n",
        "            else:\n",
        "                print(indent + f\"Split, feature {node.split_feature}, value -> {node.split_val}\")\n",
        "\n",
        "                print(indent + \" -> \" + f\"(A)\")\n",
        "                print_node(node.child_node_a, indent + \"      \")\n",
        "                \n",
        "                print(indent + \" -> \" + f\"(B)\")\n",
        "                print_node(node.child_node_b, indent + \"      \")\n",
        "        \n",
        "        print_node(self.tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U033RY1_YS8x"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'predict'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/jonczyk/WSI/ID3_ALGORITHM/id3_tree.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jonczyk/WSI/ID3_ALGORITHM/id3_tree.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dc\u001b[39m.\u001b[39mbuild_tree(x_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jonczyk/WSI/ID3_ALGORITHM/id3_tree.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample, gt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(x_test, y_test):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jonczyk/WSI/ID3_ALGORITHM/id3_tree.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     prediction \u001b[39m=\u001b[39m dc\u001b[39m.\u001b[39;49mpredict(sample)\n",
            "\u001b[1;32m/home/jonczyk/WSI/ID3_ALGORITHM/id3_tree.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jonczyk/WSI/ID3_ALGORITHM/id3_tree.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jonczyk/WSI/ID3_ALGORITHM/id3_tree.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree\u001b[39m.\u001b[39;49mpredict(data)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "dc = DecisionTreeClassifier(3)\n",
        "dc.build_tree(x_train, y_train)\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
